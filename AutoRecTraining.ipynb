{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run: Run All directly\n",
    "### When making changes on the parameters, make sure change it as well on prediction file\n",
    "### Note: Don't escape when making training and ensure the model has been saved correctly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, div, square, norm\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data and set device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_environment():  \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return device\n",
    "\n",
    "def create_user_item_ratingMatrix(df,num_users,num_items):\n",
    "    user_item_ratingMatrix = torch.zeros((num_users, num_items))\n",
    "    for row in df.itertuples():\n",
    "        user_item_ratingMatrix[row[1]-1, row[2]-1] = row[3]\n",
    "    return user_item_ratingMatrix\n",
    "\n",
    "def load_data(Upath,Mpath,Rpath):\n",
    "    num_users = pd.read_csv(Upath,delimiter=\"::\",header=None,engine='python')[0].max()\n",
    "    num_items = pd.read_csv(Mpath,delimiter=\"::\",header=None,engine='python')[0].max()\n",
    "    df_ratings = pd.read_csv(Rpath, sep='::', names=['user_id', 'MovieID', 'rating', 'timestamp'])\n",
    "    df_movies = pd.read_csv(Mpath, sep=\"::\", header=None, names=[\"MovieID\", \"Title\", \"Genres\"], engine=\"python\")\n",
    "    user_item_ratingMatrix = create_user_item_ratingMatrix(df_ratings,num_users,num_items)\n",
    "    return user_item_ratingMatrix, num_users, num_items, df_ratings, df_movies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a list of items into a PyTorch LongTensor,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return torch.LongTensor(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn the train,test,whole dataset into a DataLoader\n",
    "### Code Reference: https://github.com/tuanio/AutoRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "def Create_train_test(num_items,batch_size=512,num_workers=2):\n",
    "    \n",
    "    train_items,test_items = train_test_split(torch.arange(num_items),\n",
    "                                           test_size=0.2,\n",
    "                                           random_state=12)\n",
    "    \n",
    "    train_dl = DataLoader(train_items,shuffle=True,num_workers=num_workers,batch_size=batch_size,drop_last=True,collate_fn=collate_fn)\n",
    "    test_dl = DataLoader(test_items, shuffle=False,num_workers=num_workers,batch_size=batch_size,collate_fn=collate_fn)\n",
    "    whole_dl = DataLoader(torch.arange(num_items), shuffle=False,num_workers=1,batch_size=num_items,collate_fn=collate_fn)\n",
    "    \n",
    "    return train_dl,test_dl,whole_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoRec model\n",
    "### Reference: https://github.com/tuanio/AutoRec\n",
    "### Paper reference: http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoRec(nn.Module):\n",
    "    def __init__(self, visibleDimensions, hiddenDimensions, learningRate):\n",
    "        super().__init__()\n",
    "        self.learningRate = learningRate\n",
    "        self.weight1 = nn.Parameter(torch.randn(visibleDimensions, hiddenDimensions))\n",
    "        self.weight2 = nn.Parameter(torch.randn(hiddenDimensions, visibleDimensions))\n",
    "        self.bias1 = nn.Parameter(torch.randn(hiddenDimensions))\n",
    "        self.bias2 = nn.Parameter(torch.randn(visibleDimensions))\n",
    "    \n",
    "    def regularization(self):\n",
    "        return div(self.learningRate, 2) * (square(norm(self.weight1)) + square(norm(self.weight2)))\n",
    "    \n",
    "    def forward(self, data):\n",
    "        encoder = self.weight2.matmul(data.T).T + self.bias1\n",
    "        return self.weight1.matmul(encoder.sigmoid().T).T + self.bias2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation function, calculate RMSE\n",
    "#### Code Reference: https://github.com/tuanio/AutoRec\n",
    "#### Apply the same method to make 0 rating as -1, then put into the training, and make -1 rating as 0\n",
    "#### Return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(model, test_set, criterion):\n",
    "    model.eval()\n",
    "    truth = []\n",
    "    predict = []\n",
    "    loss = []\n",
    "    with torch.no_grad():\n",
    "        for _ ,items_idx in enumerate(test_set):\n",
    "            ratings = user_item_ratingMatrix[:, items_idx].squeeze().permute(1,0).to(device)\n",
    "            ratings[ratings==0] = -1\n",
    "            ratings_prediction = model(ratings)\n",
    "            ratings_prediction[ratings == -1] = 0\n",
    "            ratings[ratings == -1] = 0\n",
    "            truth.append(ratings)\n",
    "            predict.append(ratings_prediction * torch.sign(ratings))       \n",
    "            single_loss = criterion(ratings, ratings_prediction * torch.sign(ratings)) + model.regularization()\n",
    "            loss.append(single_loss.item())\n",
    "\n",
    "    rmse = torch.Tensor([torch.sqrt(square(ratings - ratings_prediction).sum() / torch.sign(ratings).sum())\n",
    "                            for ratings, ratings_prediction in zip(truth, predict)]).mean().item()\n",
    "    return loss, rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model\n",
    "#### Code Reference: https://github.com/tuanio/AutoRec\n",
    "#### Difference from the original code: make 0 rating as -1, then put into the training, and make -1 rating as 0\n",
    "#### Aim: to avoid the loss of 0 rating\n",
    "#### Paper reference: http://users.cecs.anu.edu.au/~u5098633/papers/www15.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model,train_set,user_item_ratingMatrix,optimizer,criterion):\n",
    "    lossList = []\n",
    "    for _ , item_idx in enumerate(train_set):\n",
    "        ratings = user_item_ratingMatrix[:,item_idx].squeeze().permute(1,0).to(device)\n",
    "        ratings[ratings == 0] = -1\n",
    "        predict_ratings = model(ratings)\n",
    "        predict_ratings[ratings == -1] = 0\n",
    "        loss = criterion(ratings, predict_ratings * torch.sign(ratings)) + model.regularization()       \n",
    "        lossList.append(loss.item())        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return lossList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model, if met the lowest RMSE will save the model\n",
    "#### Code Reference: https://github.com/tuanio/AutoRec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the environment...\n",
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vincentzhao/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train and test sets...\n"
     ]
    }
   ],
   "source": [
    "print(\"Setting up the environment...\")\n",
    "device = set_up_environment()\n",
    "print(\"Loading data...\")\n",
    "user_item_ratingMatrix, num_users, num_items, df_ratings, df_movies = load_data(Upath=\"./ml-1m/users.dat\",Mpath=\"./ml-1m/movies.dat\",Rpath=\"./ml-1m/ratings.dat\")\n",
    "\n",
    "print(\"Creating train and test sets...\")\n",
    "train_set, test_set, whole_set = Create_train_test(num_items=num_items,batch_size=32)\n",
    "\n",
    "def Train_and_Save():\n",
    "    print(\"Creating model...\")\n",
    "    model = AutoRec(visibleDimensions=num_users, hiddenDimensions=500, learningRate=0.0001).to(device)\n",
    "\n",
    "    print(\"Creating optimizer and criterion...\")\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=0.012, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "\n",
    "    print(\"Creating data loaders...\")\n",
    "    max_epochs = 100\n",
    "    losses = []\n",
    "    eval_losses = []\n",
    "    eval_rmse = []\n",
    "    min_rmse = 1000\n",
    "    \n",
    "    print(\"Training model...\")\n",
    "    for epoch_idx in range(max_epochs):\n",
    "        print(\"=\" * 10 + f\"Epoch: {epoch_idx}\" + \"=\" * 10)\n",
    "        epoch_loss = training(model,train_set,user_item_ratingMatrix,optimiser,criterion)\n",
    "        evaluation_loss, rmse = eval_epoch(model, test_set, criterion)\n",
    "        losses.extend(epoch_loss)\n",
    "        eval_losses.extend(evaluation_loss)\n",
    "        if rmse < min_rmse:\n",
    "            print(\"Saving model...\")\n",
    "            min_rmse = rmse\n",
    "            \n",
    "            # change the path to your own path, and name the model as you wish\n",
    "            torch.save(model.state_dict(), './model/AutoRec.pth')\n",
    "        eval_rmse.append(rmse)\n",
    "        print(\"Epoch Loss: \", losses[-1])\n",
    "        print(\"Evaluation Loss: \", eval_losses[-1])\n",
    "        print(\"RMSE: \", eval_rmse[-1])\n",
    "    return losses, eval_losses, eval_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "Creating optimizer and criterion...\n",
      "Creating data loaders...\n",
      "Training model...\n",
      "==========Epoch: 0==========\n",
      "Saving model...\n",
      "Epoch Loss:  83.09021759033203\n",
      "Evaluation Loss:  80.69125366210938\n",
      "RMSE:  2.756258964538574\n",
      "==========Epoch: 1==========\n",
      "Saving model...\n",
      "Epoch Loss:  22.46796417236328\n",
      "Evaluation Loss:  21.197832107543945\n",
      "RMSE:  1.4833197593688965\n",
      "==========Epoch: 2==========\n",
      "Saving model...\n",
      "Epoch Loss:  7.12447452545166\n",
      "Evaluation Loss:  6.072370529174805\n",
      "RMSE:  1.1608784198760986\n",
      "==========Epoch: 3==========\n",
      "Saving model...\n",
      "Epoch Loss:  3.02280855178833\n",
      "Evaluation Loss:  2.0235443115234375\n",
      "RMSE:  1.0861060619354248\n",
      "==========Epoch: 4==========\n",
      "Saving model...\n",
      "Epoch Loss:  1.7949788570404053\n",
      "Evaluation Loss:  0.8100163340568542\n",
      "RMSE:  1.0845259428024292\n",
      "==========Epoch: 5==========\n",
      "Epoch Loss:  1.377124309539795\n",
      "Evaluation Loss:  0.3910427391529083\n",
      "RMSE:  1.0932574272155762\n",
      "==========Epoch: 6==========\n",
      "Epoch Loss:  1.2027153968811035\n",
      "Evaluation Loss:  0.22496247291564941\n",
      "RMSE:  1.1410914659500122\n",
      "==========Epoch: 7==========\n",
      "Epoch Loss:  1.1394120454788208\n",
      "Evaluation Loss:  0.1649250090122223\n",
      "RMSE:  1.1292859315872192\n",
      "==========Epoch: 8==========\n",
      "Epoch Loss:  1.1001849174499512\n",
      "Evaluation Loss:  0.12301812320947647\n",
      "RMSE:  1.1663520336151123\n",
      "==========Epoch: 9==========\n",
      "Epoch Loss:  1.0735872983932495\n",
      "Evaluation Loss:  0.09609586000442505\n",
      "RMSE:  1.2087998390197754\n",
      "==========Epoch: 10==========\n",
      "Epoch Loss:  1.0933252573013306\n",
      "Evaluation Loss:  0.10076487809419632\n",
      "RMSE:  1.1695233583450317\n",
      "==========Epoch: 11==========\n",
      "Epoch Loss:  1.061583161354065\n",
      "Evaluation Loss:  0.07785845547914505\n",
      "RMSE:  1.1197482347488403\n",
      "==========Epoch: 12==========\n",
      "Epoch Loss:  1.065487027168274\n",
      "Evaluation Loss:  0.08105983585119247\n",
      "RMSE:  1.190457820892334\n",
      "==========Epoch: 13==========\n",
      "Epoch Loss:  1.0626777410507202\n",
      "Evaluation Loss:  0.0761970803141594\n",
      "RMSE:  1.1642332077026367\n",
      "==========Epoch: 14==========\n",
      "Epoch Loss:  1.090078353881836\n",
      "Evaluation Loss:  0.09062319993972778\n",
      "RMSE:  1.2041065692901611\n",
      "==========Epoch: 15==========\n",
      "Epoch Loss:  1.0730514526367188\n",
      "Evaluation Loss:  0.09473336488008499\n",
      "RMSE:  1.205883502960205\n",
      "==========Epoch: 16==========\n",
      "Epoch Loss:  1.0808887481689453\n",
      "Evaluation Loss:  0.09234951436519623\n",
      "RMSE:  1.1681307554244995\n",
      "==========Epoch: 17==========\n",
      "Epoch Loss:  1.0830868482589722\n",
      "Evaluation Loss:  0.10402348637580872\n",
      "RMSE:  1.2216479778289795\n",
      "==========Epoch: 18==========\n",
      "Epoch Loss:  1.075437307357788\n",
      "Evaluation Loss:  0.09902075678110123\n",
      "RMSE:  1.1962453126907349\n",
      "==========Epoch: 19==========\n",
      "Epoch Loss:  1.0682494640350342\n",
      "Evaluation Loss:  0.08538214862346649\n",
      "RMSE:  1.251766324043274\n",
      "==========Epoch: 20==========\n",
      "Epoch Loss:  1.1344733238220215\n",
      "Evaluation Loss:  0.12435750663280487\n",
      "RMSE:  1.2699708938598633\n",
      "==========Epoch: 21==========\n",
      "Epoch Loss:  1.0865154266357422\n",
      "Evaluation Loss:  0.08941663801670074\n",
      "RMSE:  1.1919363737106323\n",
      "==========Epoch: 22==========\n",
      "Epoch Loss:  1.0678279399871826\n",
      "Evaluation Loss:  0.09186513721942902\n",
      "RMSE:  1.2472056150436401\n",
      "==========Epoch: 23==========\n",
      "Epoch Loss:  1.1326020956039429\n",
      "Evaluation Loss:  0.13771399855613708\n",
      "RMSE:  1.2785075902938843\n",
      "==========Epoch: 24==========\n",
      "Epoch Loss:  1.1041394472122192\n",
      "Evaluation Loss:  0.12654894590377808\n",
      "RMSE:  1.1850923299789429\n",
      "==========Epoch: 25==========\n",
      "Epoch Loss:  1.1092684268951416\n",
      "Evaluation Loss:  0.12223710119724274\n",
      "RMSE:  1.2499405145645142\n",
      "==========Epoch: 26==========\n",
      "Epoch Loss:  1.111958622932434\n",
      "Evaluation Loss:  0.11297079175710678\n",
      "RMSE:  1.2653288841247559\n",
      "==========Epoch: 27==========\n",
      "Epoch Loss:  1.1058346033096313\n",
      "Evaluation Loss:  0.13095682859420776\n",
      "RMSE:  1.2669250965118408\n",
      "==========Epoch: 28==========\n",
      "Epoch Loss:  1.1080046892166138\n",
      "Evaluation Loss:  0.10971055179834366\n",
      "RMSE:  1.2773611545562744\n",
      "==========Epoch: 29==========\n",
      "Epoch Loss:  1.202209234237671\n",
      "Evaluation Loss:  0.15919919312000275\n",
      "RMSE:  1.2442407608032227\n",
      "==========Epoch: 30==========\n",
      "Epoch Loss:  1.141508936882019\n",
      "Evaluation Loss:  0.15279731154441833\n",
      "RMSE:  1.3595584630966187\n",
      "==========Epoch: 31==========\n",
      "Epoch Loss:  1.0905224084854126\n",
      "Evaluation Loss:  0.10962307453155518\n",
      "RMSE:  1.3105920553207397\n",
      "==========Epoch: 32==========\n",
      "Epoch Loss:  1.1257288455963135\n",
      "Evaluation Loss:  0.14282652735710144\n",
      "RMSE:  1.284483790397644\n",
      "==========Epoch: 33==========\n",
      "Epoch Loss:  1.123903512954712\n",
      "Evaluation Loss:  0.12883049249649048\n",
      "RMSE:  1.2733947038650513\n",
      "==========Epoch: 34==========\n",
      "Epoch Loss:  1.117464303970337\n",
      "Evaluation Loss:  0.13598336279392242\n",
      "RMSE:  1.2976789474487305\n",
      "==========Epoch: 35==========\n",
      "Epoch Loss:  1.1203579902648926\n",
      "Evaluation Loss:  0.13343846797943115\n",
      "RMSE:  1.287786841392517\n",
      "==========Epoch: 36==========\n",
      "Epoch Loss:  1.136720061302185\n",
      "Evaluation Loss:  0.16697731614112854\n",
      "RMSE:  1.3553804159164429\n",
      "==========Epoch: 37==========\n",
      "Epoch Loss:  1.1475441455841064\n",
      "Evaluation Loss:  0.13563860952854156\n",
      "RMSE:  1.2994025945663452\n",
      "==========Epoch: 38==========\n",
      "Epoch Loss:  1.1214933395385742\n",
      "Evaluation Loss:  0.13459791243076324\n",
      "RMSE:  1.3029987812042236\n",
      "==========Epoch: 39==========\n",
      "Epoch Loss:  1.2070485353469849\n",
      "Evaluation Loss:  0.20902785658836365\n",
      "RMSE:  1.281842827796936\n",
      "==========Epoch: 40==========\n",
      "Epoch Loss:  1.1469252109527588\n",
      "Evaluation Loss:  0.15240320563316345\n",
      "RMSE:  1.285017490386963\n",
      "==========Epoch: 41==========\n",
      "Epoch Loss:  1.194942831993103\n",
      "Evaluation Loss:  0.19933974742889404\n",
      "RMSE:  1.265599250793457\n",
      "==========Epoch: 42==========\n",
      "Epoch Loss:  1.1388740539550781\n",
      "Evaluation Loss:  0.1659919023513794\n",
      "RMSE:  1.4374467134475708\n",
      "==========Epoch: 43==========\n",
      "Epoch Loss:  1.1480953693389893\n",
      "Evaluation Loss:  0.16293802857398987\n",
      "RMSE:  1.3107563257217407\n",
      "==========Epoch: 44==========\n",
      "Epoch Loss:  1.150538444519043\n",
      "Evaluation Loss:  0.15936164557933807\n",
      "RMSE:  1.2579598426818848\n",
      "==========Epoch: 45==========\n",
      "Epoch Loss:  1.129857063293457\n",
      "Evaluation Loss:  0.15246199071407318\n",
      "RMSE:  1.3639134168624878\n",
      "==========Epoch: 46==========\n",
      "Epoch Loss:  1.1037266254425049\n",
      "Evaluation Loss:  0.14300325512886047\n",
      "RMSE:  1.4409898519515991\n",
      "==========Epoch: 47==========\n",
      "Epoch Loss:  1.1661105155944824\n",
      "Evaluation Loss:  0.14642183482646942\n",
      "RMSE:  1.2740373611450195\n",
      "==========Epoch: 48==========\n",
      "Epoch Loss:  1.191208004951477\n",
      "Evaluation Loss:  0.18417814373970032\n",
      "RMSE:  1.3081769943237305\n",
      "==========Epoch: 49==========\n",
      "Epoch Loss:  1.1281074285507202\n",
      "Evaluation Loss:  0.15136411786079407\n",
      "RMSE:  1.3372622728347778\n",
      "==========Epoch: 50==========\n",
      "Epoch Loss:  1.1232422590255737\n",
      "Evaluation Loss:  0.12961211800575256\n",
      "RMSE:  1.4080216884613037\n",
      "==========Epoch: 51==========\n",
      "Epoch Loss:  1.1433286666870117\n",
      "Evaluation Loss:  0.16644494235515594\n",
      "RMSE:  1.3667036294937134\n",
      "==========Epoch: 52==========\n",
      "Epoch Loss:  1.141689658164978\n",
      "Evaluation Loss:  0.1828562617301941\n",
      "RMSE:  1.322092890739441\n",
      "==========Epoch: 53==========\n",
      "Epoch Loss:  1.1680026054382324\n",
      "Evaluation Loss:  0.18993878364562988\n",
      "RMSE:  1.3183457851409912\n",
      "==========Epoch: 54==========\n",
      "Epoch Loss:  1.1130855083465576\n",
      "Evaluation Loss:  0.12527623772621155\n",
      "RMSE:  1.3221054077148438\n",
      "==========Epoch: 55==========\n",
      "Epoch Loss:  1.16763436794281\n",
      "Evaluation Loss:  0.15359538793563843\n",
      "RMSE:  1.3384888172149658\n",
      "==========Epoch: 56==========\n",
      "Epoch Loss:  1.1351401805877686\n",
      "Evaluation Loss:  0.13869979977607727\n",
      "RMSE:  1.4206002950668335\n",
      "==========Epoch: 57==========\n",
      "Epoch Loss:  1.1571732759475708\n",
      "Evaluation Loss:  0.17170807719230652\n",
      "RMSE:  1.4008762836456299\n",
      "==========Epoch: 58==========\n",
      "Epoch Loss:  1.1531603336334229\n",
      "Evaluation Loss:  0.16703540086746216\n",
      "RMSE:  1.3743751049041748\n",
      "==========Epoch: 59==========\n",
      "Epoch Loss:  1.1973518133163452\n",
      "Evaluation Loss:  0.22184297442436218\n",
      "RMSE:  1.3649417161941528\n",
      "==========Epoch: 60==========\n",
      "Epoch Loss:  1.2147618532180786\n",
      "Evaluation Loss:  0.2258116453886032\n",
      "RMSE:  1.3054922819137573\n",
      "==========Epoch: 61==========\n",
      "Epoch Loss:  1.1419812440872192\n",
      "Evaluation Loss:  0.14854779839515686\n",
      "RMSE:  1.3278497457504272\n",
      "==========Epoch: 62==========\n",
      "Epoch Loss:  1.1714893579483032\n",
      "Evaluation Loss:  0.19125241041183472\n",
      "RMSE:  1.2861709594726562\n",
      "==========Epoch: 63==========\n",
      "Epoch Loss:  1.1415144205093384\n",
      "Evaluation Loss:  0.15585672855377197\n",
      "RMSE:  1.3305937051773071\n",
      "==========Epoch: 64==========\n",
      "Epoch Loss:  1.170461654663086\n",
      "Evaluation Loss:  0.1829048991203308\n",
      "RMSE:  1.4018020629882812\n",
      "==========Epoch: 65==========\n",
      "Epoch Loss:  1.2528553009033203\n",
      "Evaluation Loss:  0.24774691462516785\n",
      "RMSE:  1.3925453424453735\n",
      "==========Epoch: 66==========\n",
      "Epoch Loss:  1.1758228540420532\n",
      "Evaluation Loss:  0.18053969740867615\n",
      "RMSE:  1.288049578666687\n",
      "==========Epoch: 67==========\n",
      "Epoch Loss:  1.1788618564605713\n",
      "Evaluation Loss:  0.20756280422210693\n",
      "RMSE:  1.344336986541748\n",
      "==========Epoch: 68==========\n",
      "Epoch Loss:  1.1953333616256714\n",
      "Evaluation Loss:  0.18414844572544098\n",
      "RMSE:  1.41689133644104\n",
      "==========Epoch: 69==========\n",
      "Epoch Loss:  1.155785083770752\n",
      "Evaluation Loss:  0.16027167439460754\n",
      "RMSE:  1.3473888635635376\n",
      "==========Epoch: 70==========\n",
      "Epoch Loss:  1.1342262029647827\n",
      "Evaluation Loss:  0.1474883109331131\n",
      "RMSE:  1.4107732772827148\n",
      "==========Epoch: 71==========\n",
      "Epoch Loss:  1.2043836116790771\n",
      "Evaluation Loss:  0.18700657784938812\n",
      "RMSE:  1.3245621919631958\n",
      "==========Epoch: 72==========\n",
      "Epoch Loss:  1.2129549980163574\n",
      "Evaluation Loss:  0.21646134555339813\n",
      "RMSE:  1.3657526969909668\n",
      "==========Epoch: 73==========\n",
      "Epoch Loss:  1.1445610523223877\n",
      "Evaluation Loss:  0.15267428755760193\n",
      "RMSE:  1.353929042816162\n",
      "==========Epoch: 74==========\n",
      "Epoch Loss:  1.1340688467025757\n",
      "Evaluation Loss:  0.15677765011787415\n",
      "RMSE:  1.339760184288025\n",
      "==========Epoch: 75==========\n",
      "Epoch Loss:  1.2567577362060547\n",
      "Evaluation Loss:  0.24559848010540009\n",
      "RMSE:  1.303621530532837\n",
      "==========Epoch: 76==========\n",
      "Epoch Loss:  1.1742838621139526\n",
      "Evaluation Loss:  0.18702727556228638\n",
      "RMSE:  1.3975735902786255\n",
      "==========Epoch: 77==========\n",
      "Epoch Loss:  1.1395944356918335\n",
      "Evaluation Loss:  0.15931342542171478\n",
      "RMSE:  1.3480169773101807\n",
      "==========Epoch: 78==========\n",
      "Epoch Loss:  1.1577956676483154\n",
      "Evaluation Loss:  0.16331319510936737\n",
      "RMSE:  1.3110226392745972\n",
      "==========Epoch: 79==========\n",
      "Epoch Loss:  1.170738935470581\n",
      "Evaluation Loss:  0.1622447371482849\n",
      "RMSE:  1.3453378677368164\n",
      "==========Epoch: 80==========\n",
      "Epoch Loss:  1.1896642446517944\n",
      "Evaluation Loss:  0.15491586923599243\n",
      "RMSE:  1.3611778020858765\n",
      "==========Epoch: 81==========\n",
      "Epoch Loss:  1.1606130599975586\n",
      "Evaluation Loss:  0.1738596111536026\n",
      "RMSE:  1.357090711593628\n",
      "==========Epoch: 82==========\n",
      "Epoch Loss:  1.1784405708312988\n",
      "Evaluation Loss:  0.17155545949935913\n",
      "RMSE:  1.292244553565979\n",
      "==========Epoch: 83==========\n",
      "Epoch Loss:  1.1335959434509277\n",
      "Evaluation Loss:  0.14270520210266113\n",
      "RMSE:  1.3838756084442139\n",
      "==========Epoch: 84==========\n",
      "Epoch Loss:  1.160746455192566\n",
      "Evaluation Loss:  0.1616007387638092\n",
      "RMSE:  1.3435384035110474\n",
      "==========Epoch: 85==========\n",
      "Epoch Loss:  1.1436946392059326\n",
      "Evaluation Loss:  0.1537882536649704\n",
      "RMSE:  1.2393242120742798\n",
      "==========Epoch: 86==========\n",
      "Epoch Loss:  1.1946451663970947\n",
      "Evaluation Loss:  0.19119957089424133\n",
      "RMSE:  1.3601449728012085\n",
      "==========Epoch: 87==========\n",
      "Epoch Loss:  1.1492831707000732\n",
      "Evaluation Loss:  0.17115993797779083\n",
      "RMSE:  1.4252568483352661\n",
      "==========Epoch: 88==========\n",
      "Epoch Loss:  1.1519358158111572\n",
      "Evaluation Loss:  0.14650489389896393\n",
      "RMSE:  1.2949904203414917\n",
      "==========Epoch: 89==========\n",
      "Epoch Loss:  1.136519432067871\n",
      "Evaluation Loss:  0.1523408591747284\n",
      "RMSE:  1.375683307647705\n",
      "==========Epoch: 90==========\n",
      "Epoch Loss:  1.207756519317627\n",
      "Evaluation Loss:  0.21223540604114532\n",
      "RMSE:  1.3337665796279907\n",
      "==========Epoch: 91==========\n",
      "Epoch Loss:  1.18498957157135\n",
      "Evaluation Loss:  0.18805618584156036\n",
      "RMSE:  1.2930537462234497\n",
      "==========Epoch: 92==========\n",
      "Epoch Loss:  1.1609266996383667\n",
      "Evaluation Loss:  0.17058584094047546\n",
      "RMSE:  1.3145952224731445\n",
      "==========Epoch: 93==========\n",
      "Epoch Loss:  1.2070918083190918\n",
      "Evaluation Loss:  0.2105502486228943\n",
      "RMSE:  1.4272964000701904\n",
      "==========Epoch: 94==========\n",
      "Epoch Loss:  1.1690788269042969\n",
      "Evaluation Loss:  0.18943628668785095\n",
      "RMSE:  1.3151576519012451\n",
      "==========Epoch: 95==========\n",
      "Epoch Loss:  1.1891226768493652\n",
      "Evaluation Loss:  0.18766166269779205\n",
      "RMSE:  1.350996971130371\n",
      "==========Epoch: 96==========\n",
      "Epoch Loss:  1.1736173629760742\n",
      "Evaluation Loss:  0.17127785086631775\n",
      "RMSE:  1.4368008375167847\n",
      "==========Epoch: 97==========\n",
      "Epoch Loss:  1.1604129076004028\n",
      "Evaluation Loss:  0.19524428248405457\n",
      "RMSE:  1.3233287334442139\n",
      "==========Epoch: 98==========\n",
      "Epoch Loss:  1.166318416595459\n",
      "Evaluation Loss:  0.18587881326675415\n",
      "RMSE:  1.3160033226013184\n",
      "==========Epoch: 99==========\n",
      "Epoch Loss:  1.1459165811538696\n",
      "Evaluation Loss:  0.15437352657318115\n",
      "RMSE:  1.3024193048477173\n"
     ]
    }
   ],
   "source": [
    "losses, eval_losses,eval_rmse = Train_and_Save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "caedfd20fb2d9c8a893d176d9b3154c0254ab38b07a767eab39c2054422a8676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
